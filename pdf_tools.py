# analysis_tools.py
# (PDF ë¶„ì„ + íŠ¸ë Œë“œ ë¶„ì„ì„ ë‹´ë‹¹í•˜ëŠ” 'ë„êµ¬' ëª¨ìŒ)

import fitz  # PyMuPDF
import openai
import os
from dotenv import load_dotenv
from pytrends.request import TrendReq
import pandas as pd
import json
import requests
from bs4 import BeautifulSoup

# --- OpenAI API í‚¤ ì„¤ì • (ì´ ëª¨ë“ˆë„ AIë¥¼ ì“°ë‹ˆê¹Œ) ---
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    print("âŒ [analysis_tools] OPENAI_API_KEYë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
else:
    openai.api_key = api_key

# ----------------------------------------------------
# ê¸°ëŠ¥ 1: PDF ë¶„ì„ê¸° (ai_summary_test.pyì—ì„œ ê°€ì ¸ì˜´)
# ----------------------------------------------------
def analyze_pdf(pdf_file_path):
    """
    PDF íŒŒì¼ ê²½ë¡œë¥¼ ë°›ì•„ì„œ, AIë¡œ ìš”ì•½í•œ JSONì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    print(f"  [analysis_tools] 1. PDF ë¶„ì„ ì‹œì‘: {pdf_file_path}")
    
    full_text = ""
    if not os.path.exists(pdf_file_path):
        print(f"    âŒ ì˜¤ë¥˜: '{pdf_file_path}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return {"error": "PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}

    try:
        # 1. PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
        doc = fitz.open(pdf_file_path)
        for page_num in range(doc.page_count):
            page = doc.load_page(page_num)
            full_text += page.get_text("text")
        doc.close()
        print(f"    - PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ. (ì´ {len(full_text)}ì)")

        # 2. AIì—ê²Œ ìš”ì•½ ìš”ì²­ (v3 í”„ë¡¬í”„íŠ¸ ì‚¬ìš©)
        system_prompt = """
        ë‹¹ì‹ ì€ ì¶•ì œ ê¸°íšì„œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
        (ì´í•˜ ìƒëµ ... ì´ì „ v3 í”„ë¡¬í”„íŠ¸ ë‚´ìš© ... )
        ë§Œì•½ í…ìŠ¤íŠ¸ì—ì„œ íŠ¹ì • ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´, í•´ë‹¹ ê°’ì€ "ì •ë³´ ì—†ìŒ"ìœ¼ë¡œ í‘œê¸°í•˜ì„¸ìš”.
        """
        # (â€» v3 í”„ë¡¬í”„íŠ¸ ì „ì²´ ë‚´ìš©ì„ ì—¬ê¸°ì— ë¶™ì—¬ë„£ì–´ ì£¼ì„¸ìš”!)
        
        user_prompt = f"ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ JSONìœ¼ë¡œ ìš”ì•½í•´ì¤˜:\n\n{full_text[:15000]}"

        client = openai.OpenAI()
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            response_format={"type": "json_object"}
        )
        
        ai_response_json_string = response.choices[0].message.content
        print("    - AI ìš”ì•½ ì™„ë£Œ.")
        
        # JSON ë¬¸ìì—´ì„ Python ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜í•´ì„œ ë°˜í™˜
        return json.loads(ai_response_json_string) 

    except Exception as e:
        print(f"    âŒ PDF ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return {"error": f"PDF ë¶„ì„ ì˜¤ë¥˜: {e}"}

# ----------------------------------------------------
# ê¸°ëŠ¥ 2: íŠ¸ë Œë“œ ë¶„ì„ê¸° (trend_test.pyì—ì„œ ê°€ì ¸ì˜´)
# ----------------------------------------------------
def get_google_trends(keywords_list):
    """
    í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„ì„œ, Google íŠ¸ë Œë“œ ë°ì´í„°ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    print(f"  [analysis_tools] 2. Google íŠ¸ë Œë“œ ë¶„ì„ ì‹œì‘: {keywords_list}")
    
    try:
        pytrends = TrendReq(hl='ko-KR', tz=540)
        pytrends.build_payload(keywords_list, cat=0, timeframe='today 12-m', geo='KR')
        
        # (1) ì‹œê°„ë³„ ê´€ì‹¬ë„
        interest_df = pytrends.interest_over_time()
        
        # (2) ì—°ê´€ ê²€ìƒ‰ì–´
        related_queries_dict = pytrends.related_queries()
        
        print("    - íŠ¸ë Œë“œ ë¶„ì„ ì™„ë£Œ.")
        
        # (â€» DataFrameì€ JSONìœ¼ë¡œ ë°”ë¡œ ë³´ë‚´ê¸° ê¹Œë‹¤ë¡œìš°ë¯€ë¡œ,
        #    ë‚˜ì¤‘ì— í•„ìš”í•œ 'ì—°ê´€ ê²€ìƒ‰ì–´'ë§Œ ë¨¼ì € ê°€ê³µí•´ì„œ ë°˜í™˜í•©ë‹ˆë‹¤.)
        
        top_related = {}
        for kw in keywords_list:
            top_queries = related_queries_dict.get(kw, {}).get('top')
            if top_queries is not None and not top_queries.empty:
                # 'query' ì»¬ëŸ¼ì˜ ìƒìœ„ 5ê°œë§Œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
                top_related[kw] = top_queries['query'].head(5).tolist()
            else:
                top_related[kw] = []

        return {
            "analyzed_keywords": keywords_list,
            "top_related_queries": top_related
            # "interest_data": interest_df.to_dict() # (í•„ìš”í•˜ë‹¤ë©´ ë‚˜ì¤‘ì— ì¶”ê°€)
        }

    except Exception as e:
        # (429 ì˜¤ë¥˜ ë“±ì´ ë°œìƒí•  ìˆ˜ ìˆìŒ)
        print(f"    âŒ íŠ¸ë Œë“œ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return {"error": f"íŠ¸ë Œë“œ ë¶„ì„ ì˜¤ë¥˜: {e}"}
    
# ----------------------------------------------------
# ê¸°ëŠ¥ 3: íŠ¸ë Œë“œ ë¶„ì„ê¸° (trend_test.pyì—ì„œ ê°€ì ¸ì˜´)
# ----------------------------------------------------
# analysis_tools.py íŒŒì¼ì— ì´ì–´ì„œ ì¶”ê°€í•˜ëŠ” í•¨ìˆ˜

def get_naver_buzzwords(keyword):
    """
    ë„¤ì´ë²„ VIEW(ë¸”ë¡œê·¸/ì¹´í˜) íƒ­ì„ í¬ë¡¤ë§í•˜ì—¬
    'í•¨ê»˜ ì°¾ëŠ” í‚¤ì›Œë“œ' (ì—°ê´€ íƒœê·¸) ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    print(f"  [analysis_tools] 3. Naver VIEW íƒ­ ì—°ê´€ í‚¤ì›Œë“œ ë¶„ì„ ì‹œì‘: {keyword}")
    
    # 1. ë„¤ì´ë²„ VIEW íƒ­ ê²€ìƒ‰ URL
    # (where=viewëŠ” ë¸”ë¡œê·¸/ì¹´í˜ íƒ­ì„ ì˜ë¯¸)
    url = f"https://search.naver.com/search.naver?where=view&sm=tab_jum&query={keyword}"
    
    # 2. (â­ï¸ì¤‘ìš”) í¬ë¡¤ë§ ì°¨ë‹¨ì„ í”¼í•˜ê¸° ìœ„í•œ 'User-Agent' í—¤ë” ì„¤ì •
    # (ìš°ë¦¬ê°€ 'ë¸Œë¼ìš°ì €'ì¸ ì²™ ì ‘ì†í•©ë‹ˆë‹¤)
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }

    try:
        # 3. 'requests'ë¡œ HTML í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status() # 200(ì„±ê³µ) ì½”ë“œê°€ ì•„ë‹ˆë©´ ì˜¤ë¥˜ ë°œìƒ
        
        # 4. 'BeautifulSoup'ë¡œ HTML íŒŒì‹±(ë¶„ì„) ì¤€ë¹„
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # 5. (â­ï¸ê°€ì¥ ì¤‘ìš”/ì·¨ì•½) "í•¨ê»˜ ì°¾ëŠ” í‚¤ì›Œë“œ"ê°€ ìˆëŠ” ì˜ì—­ ì°¾ê¸°
        #    ë„¤ì´ë²„ëŠ” ì´ CSS ì„ íƒì(selector)ë¥¼ ìì£¼ ë°”ê¿‰ë‹ˆë‹¤.
        #    '.keyword_box_wrap .keyword' ë˜ëŠ” '.total_tag_area .link_tag' ë“±ì„ ì‹œë„í•©ë‹ˆë‹¤.
        related_tags_elements = soup.select('.keyword_box_wrap .keyword')
        
        if not related_tags_elements:
            # ë§Œì•½ ìœ„ ì„ íƒìê°€ ì‘ë™ ì•ˆ í•˜ë©´, 'ì—°ê´€ íƒœê·¸' ì˜ì—­ì„ ì‹œë„
            related_tags_elements = soup.select('.total_tag_area .link_tag')

        buzzwords = []
        for tag_element in related_tags_elements:
            # íƒœê·¸ì—ì„œ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
            buzzword = tag_element.get_text(strip=True)
            # '#ê´‘ì£¼ë§›ì§‘' ê°™ì€ # ê¸°í˜¸ ì œê±° (ì„ íƒ ì‚¬í•­)
            buzzwords.append(buzzword.replace('#', ''))
            
        if not buzzwords:
            print("    - (ì°¸ê³ ) ì—°ê´€ í‚¤ì›Œë“œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (ë„¤ì´ë²„ êµ¬ì¡°ê°€ ë³€ê²½ë˜ì—ˆê±°ë‚˜ í‚¤ì›Œë“œ ë°ì´í„°ê°€ ì—†ìŒ)")
            return []

        print(f"    - Naver ì—°ê´€ í‚¤ì›Œë“œ ìˆ˜ì§‘ ì™„ë£Œ: {buzzwords[:5]}...") # (ë¡œê·¸ì—ëŠ” 5ê°œë§Œ)
        
        # ì¤‘ë³µ ì œê±° í›„ ìƒìœ„ 10ê°œë§Œ ë°˜í™˜
        return list(dict.fromkeys(buzzwords))[:10]

    except Exception as e:
        print(f"    âŒ Naver í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []
# --- (ì´ íŒŒì¼ ìì²´ë¥¼ í…ŒìŠ¤íŠ¸í•˜ê¸° ìœ„í•œ ì½”ë“œ) ---
if __name__ == "__main__":
    import json
    
    print("--- ğŸš€ 'analysis_tools.py' íŒŒì¼ ë‹¨ë… í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ---")
    
    # 1. PDF ë¶„ì„ í…ŒìŠ¤íŠ¸
    pdf_result = analyze_pdf("sample_plan.pdf")
    print("\n[PDF ë¶„ì„ ê²°ê³¼ (JSON)]")
    print(json.dumps(pdf_result, indent=2, ensure_ascii=False))
    
    # 2. íŠ¸ë Œë“œ ë¶„ì„ í…ŒìŠ¤íŠ¸
    trend_result = get_google_trends(["ë‹´ì–‘ ì‚°íƒ€ ì¶•ì œ", "í¬ë¦¬ìŠ¤ë§ˆìŠ¤",])
    print("\n[íŠ¸ë Œë“œ ë¶„ì„ ê²°ê³¼ (ë”•ì…”ë„ˆë¦¬)]")
    print(json.dumps(trend_result, indent=2, ensure_ascii=False))