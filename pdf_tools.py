import fitz  # PyMuPDF
import openai
import os
from dotenv import load_dotenv
from pytrends.request import TrendReq
import pandas as pd
import json
import requests
from bs4 import BeautifulSoup

import docx  # .docx 
import olefile # .hwp
import hwp5

# --- OpenAI API í‚¤ ì„¤ì • (ì´ ëª¨ë“ˆë„ AIë¥¼ ì“°ë‹ˆê¹Œ) ---
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    print("[analysis_tools] OPENAI_API_KEYë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
else:
    openai.api_key = api_key

# ----------------------------------------------------
# ê¸°ëŠ¥ 1: ë¬¸ì„œ ë¶„ì„ê¸°
# ----------------------------------------------------
def analyze_pdf(pdf_file_path):
    """
    PDF, DOCX, HWP íŒŒì¼ ê²½ë¡œë¥¼ ë°›ì•„ì„œ, AIë¡œ ìš”ì•½í•œ JSONì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    print(f"  [analysis_tools] 1. PDF ë¶„ì„ ì‹œì‘: {pdf_file_path}")
    
    full_text = ""
    if not os.path.exists(pdf_file_path):
        print(f" ì˜¤ë¥˜: '{pdf_file_path}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return {"error": "PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}

    try:
        print(f"íŒŒì¼ íƒ€ì… ê°ì§€ ì¤‘...")
        file_extension = os.path.splitext(pdf_file_path)[1].lower()

        if file_extension == '.pdf':
            print("PDF íŒŒì¼ ê°ì§€. PyMuPDFë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ...")
            doc = fitz.open(pdf_file_path)
            for page_num in range(doc.page_count):
                page = doc.load_page(page_num)
                full_text += page.get_text("text")
            doc.close()
        
        elif file_extension == '.docx':
            print(" DOCX íŒŒì¼ ê°ì§€. python-docxë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ...")
            doc = docx.Document(pdf_file_path)
            for para in doc.paragraphs:
                full_text += para.text + "\n"
        
        elif file_extension == '.hwp':
            print(" HWP íŒŒì¼ ê°ì§€. pyhwp/hwp5ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ...")
            try:
                # (HWPëŠ” ë³µì¡í•´ì„œ 2ê°€ì§€ ë°©ì‹ì„ ëª¨ë‘ ì‹œë„)
                
                # ë°©ì‹ 1: HWP5 ë¼ì´ë¸ŒëŸ¬ë¦¬ (ìµœì‹  HWP)
                from hwp5.hwp5txt import Hwp5Txt
                h = Hwp5Txt(pdf_file_path)
                full_text = h.get_text()
                
                if not full_text: # í…ìŠ¤íŠ¸ ì¶”ì¶œì´ ì•ˆ ë˜ë©´ ë°©ì‹ 2 ì‹œë„
                    raise Exception("HWP5 íŒŒì„œê°€ í…ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•˜ì§€ ì•ŠìŒ")
                    
            except Exception as hwp5_e:
                print(f"    - HWP5 íŒŒì„œ ì‹¤íŒ¨ ({hwp5_e}). OLE 'PrvText' ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ ì¬ì‹œë„...")
                try:
                    # ë°©ì‹ 2: OLE 'ë¯¸ë¦¬ë³´ê¸° í…ìŠ¤íŠ¸' (êµ¬í˜• HWP)
                    f = olefile.OleFileIO(pdf_file_path)
                    encoded_text = f.openstream('PrvText').read()
                    full_text = encoded_text.decode('euc-kr', errors='ignore')
                except Exception as ole_e:
                    print(f" HWP íŒŒì¼ 'PrvText' ìŠ¤íŠ¸ë¦¼ ì½ê¸° ì‹¤íŒ¨: {ole_e}")
                    return {"error": "HWP íŒŒì¼ í…ìŠ¤íŠ¸ ì¶”ì¶œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. (ì§€ì›ë˜ì§€ ì•ŠëŠ” HWP ë²„ì „ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤)"}
        
        else:
            print(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤: {file_extension}")
            return {"error": f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_extension}. (PDF, DOCX, HWPë§Œ ì§€ì›)"}


        print(f" í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ. (ì´ {len(full_text)}ì)")

        # 2. AIì—ê²Œ ìš”ì•½ ìš”ì²­ 
        system_prompt = """
        ë‹¹ì‹ ì€ ì¶•ì œ ê¸°íšì„œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
        ì‚¬ìš©ìê°€ ì œê³µí•˜ëŠ” ê¸°íšì„œ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬,
        ì•„ë˜ í•­ëª©ì— í•´ë‹¹í•˜ëŠ” 'êµ¬ì²´ì ì¸ ìƒì„¸ ë‚´ìš©'ì„ ì¶”ì¶œí•˜ê³ 
        ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”.
        
        [ì¤‘ìš” ê·œì¹™]
        1. ì˜¤ì§ ì•„ë˜ ëª©ë¡ì—ì„œ ìš”ì²­ëœ í•­ëª©('title', 'date', 'location' ë“±)ë§Œ ì¶”ì¶œí•˜ì„¸ìš”.
        2. 'ì˜ˆì‚°', 'ì‚¬ì—…ë¹„', 'ì´ê¸ˆì•¡' ë“± **ê¸ˆì•¡(ëˆ)ê³¼ ê´€ë ¨ëœ ëª¨ë“  ì •ë³´**ëŠ” 
           ê·¸ê²ƒì´ ì–´ë–¤ í•­ëª©ì´ë“  **ì ˆëŒ€ë¡œ** ìš”ì•½ì— í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
        3. 'ì•ˆì „ ëŒ€ì±…(Safety Measures)', 'í–‰ì • ì‚¬í•­', 'ì…ì°°' ë“± 
           ëª©ë¡ì— ì—†ëŠ” ë‹¤ë¥¸ ì •ë³´ë„ **ì ˆëŒ€ë¡œ** ìš”ì•½ì— í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
        
        --- (ì¶”ì¶œí•  í•­ëª© ëª©ë¡) ---
        - "title": ì¶•ì œ ê³µì‹ ì œëª©
        - "date": ì¶•ì œê°€ ì—´ë¦¬ëŠ” ì •í™•í•œ ë‚ ì§œì™€ ê¸°ê°„
        - "location": ì¶•ì œê°€ ì—´ë¦¬ëŠ” êµ¬ì²´ì ì¸ ì¥ì†Œ
        - "host": ì£¼ìµœ ê¸°ê´€
        - "organizer": ì£¼ê´€ ê¸°ê´€
        - "targetAudience": ì¶•ì œì˜ ì£¼ìš” ëŒ€ìƒ ê³ ê° (ì˜ˆ: 'ê°€ì¡± ë‹¨ìœ„ ë°©ë¬¸ê°', '2030 ì—°ì¸', 'ì–´ë¦°ì´'). 'ì£¼ìš” íƒ€ê¹ƒ' ë˜ëŠ” 'ê³ ê°ì¸µ' ê°™ì€ ë‹¨ì–´ ê·¼ì²˜ë¥¼ ì°¾ì•„ë³´ì„¸ìš”.
        - "summary": ì¶•ì œì˜ ëª©ì ê³¼ í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½
        - "programs": ë°©ë¬¸ê°ì´ 'ì²´í—˜'í•  ìˆ˜ ìˆëŠ” ì£¼ìš” í”„ë¡œê·¸ë¨ì˜ 'êµ¬ì²´ì ì¸ ë‚´ìš©' (ë¦¬ìŠ¤íŠ¸). (ì£¼ì˜: 'í”„ë¡œê·¸ë¨'ì´ë¼ëŠ” ì œëª©ì˜ ëª©ì°¨ë¿ë§Œ ì•„ë‹ˆë¼, ê·¸ 'ìƒì„¸ ë‚´ìš©'ì„ ì°¾ì•„ì£¼ì„¸ìš”.)
        - "events": ì¶•ì œ ê¸°ê°„ ì¤‘ ì—´ë¦¬ëŠ” 'íŠ¹ë³„ ì´ë²¤íŠ¸'ì˜ 'êµ¬ì²´ì ì¸ ë‚´ìš©' (ë¦¬ìŠ¤íŠ¸). (ì˜ˆ: 'ê°œë§‰ í¼í¬ë¨¼ìŠ¤', 'ì‚°íƒ€ ì´ë²¤íŠ¸ ìš´ì˜'). (ì£¼ì˜: 'ì´ë²¤íŠ¸'ë¼ëŠ” ì œëª©ì˜ ëª©ì°¨ë¿ë§Œ ì•„ë‹ˆë¼, ê·¸ 'ìƒì„¸ ë‚´ìš©'ì„ ì°¾ì•„ì£¼ì„¸ìš”.)
        - "visualKeywords": ì¹´ë“œë‰´ìŠ¤ ë””ìì¸ì— ì°¸ê³ í•  ë§Œí•œ ì‹œê°ì  í‚¤ì›Œë“œ (ì˜ˆ: "ì•¼ê°„ ì¡°ëª…", "í¬ë¦¬ìŠ¤ë§ˆìŠ¤ íŠ¸ë¦¬", "ì‚°íƒ€") (ë¦¬ìŠ¤íŠ¸)
        - "contactInfo": ë°©ë¬¸ê°ì´ ë¬¸ì˜í•  ìˆ˜ ìˆëŠ” ì „í™”ë²ˆí˜¸ ë˜ëŠ” ê³µì‹ ì›¹ì‚¬ì´íŠ¸ ì£¼ì†Œ
        - "directions": ë°©ë¬¸ê°ì´ ì¶•ì œ ì¥ì†Œì— 'ì˜¤ì‹œëŠ” ê¸¸' (ì˜ˆ: 'xx ICì—ì„œ 10ë¶„', 'ë‹´ì–‘ ë²„ìŠ¤í„°ë¯¸ë„ì—ì„œ 5ë²ˆ ë²„ìŠ¤', 'ì£¼ì°¨: ë©”íƒ€ëœë“œ ì£¼ì°¨ì¥ ì´ìš©').
                       (ì£¼ì˜: 'ì‚¬ì—… ì§€ì‹œ'ë‚˜ 'ì œì•ˆì„œ ì ‘ìˆ˜' ë‚´ìš©ì´ ì•„ë‹˜. ë°©ë¬¸ê°ìš© êµí†µ/ì£¼ì°¨ ì •ë³´ê°€ ëª…í™•íˆ ì—†ìœ¼ë©´ "ì •ë³´ ì—†ìŒ"ìœ¼ë¡œ í‘œê¸°)
        
        ë§Œì•½ í…ìŠ¤íŠ¸ì—ì„œ íŠ¹ì • ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´, í•´ë‹¹ ê°’ì€ "ì •ë³´ ì—†ìŒ"ìœ¼ë¡œ í‘œê¸°í•˜ì„¸ìš”.

        [ìµœì¢… í™•ì¸ ê·œì¹™]
        ì‘ë‹µí•˜ê¸° ì „, ë‹¹ì‹ ì´ ìƒì„±í•œ JSONì„ ë‹¤ì‹œ í•œë²ˆ í™•ì¸í•˜ì„¸ìš”.
        JSON ë‚´ë¶€ì— 'ì˜ˆì‚°', 'ì‚¬ì—…ë¹„' ë“± **ê¸ˆì•¡(ëˆ)ê³¼ ê´€ë ¨ëœ ë‚´ìš©**ì´ë‚˜, 
        'ì•ˆì „ ëŒ€ì±…' ë“± --- (ì¶”ì¶œí•  í•­ëª© ëª©ë¡) ---ì— ì—†ì—ˆë˜ í•­ëª©ì´ í¬í•¨ë˜ì–´ ìˆë‚˜ìš”?
        ë§Œì•½ ê·¸ë ‡ë‹¤ë©´, ê·¸ í•­ëª©ë“¤ì„ **ë°˜ë“œì‹œ ì‚­ì œ**í•˜ê³ 
        ì˜¤ì§ 'title'ë¶€í„° 'directions'ê¹Œì§€ì˜ í•­ëª©ë§Œ í¬í•¨í•´ì„œ ì‘ë‹µí•˜ì„¸ìš”.
        """
        
        user_prompt = f"ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ JSONìœ¼ë¡œ ìš”ì•½í•´ì¤˜:\n\n{full_text[:10000]}"

        client = openai.OpenAI()
        response = client.chat.completions.create(
            model="gpt-4-turbo",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            response_format={"type": "json_object"},
            max_tokens= 4000
        )
        
        ai_response_json_string = response.choices[0].message.content
        print("    - AI ìš”ì•½ ì™„ë£Œ.")
        
        # JSON ë¬¸ìì—´ì„ Python ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜í•´ì„œ ë°˜í™˜
        return json.loads(ai_response_json_string) 

    except Exception as e:
        print(f" PDF ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return {"error": f"PDF ë¶„ì„ ì˜¤ë¥˜: {e}"}

# # ----------------------------------------------------
# # ê¸°ëŠ¥ 2: íŠ¸ë Œë“œ ë¶„ì„ê¸° (trend_test.pyì—ì„œ ê°€ì ¸ì˜´)
# # ----------------------------------------------------
# def get_google_trends(keywords_list):
#     """
#     í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„ì„œ, Google íŠ¸ë Œë“œ ë°ì´í„°ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
#     """
#     print(f"  [analysis_tools] 2. Google íŠ¸ë Œë“œ ë¶„ì„ ì‹œì‘: {keywords_list}")
    
#     try:
#         pytrends = TrendReq(hl='ko-KR', tz=540)
#         pytrends.build_payload(keywords_list, cat=0, timeframe='today 12-m', geo='KR')
        
#         # (1) ì‹œê°„ë³„ ê´€ì‹¬ë„
#         interest_df = pytrends.interest_over_time()
        
#         # (2) ì—°ê´€ ê²€ìƒ‰ì–´
#         related_queries_dict = pytrends.related_queries()
        
#         print("    - íŠ¸ë Œë“œ ë¶„ì„ ì™„ë£Œ.")
        
#         # (â€» DataFrameì€ JSONìœ¼ë¡œ ë°”ë¡œ ë³´ë‚´ê¸° ê¹Œë‹¤ë¡œìš°ë¯€ë¡œ,
#         #    ë‚˜ì¤‘ì— í•„ìš”í•œ 'ì—°ê´€ ê²€ìƒ‰ì–´'ë§Œ ë¨¼ì € ê°€ê³µí•´ì„œ ë°˜í™˜í•©ë‹ˆë‹¤.)
        
#         top_related = {}
#         for kw in keywords_list:
#             top_queries = related_queries_dict.get(kw, {}).get('top')
#             if top_queries is not None and not top_queries.empty:
#                 # 'query' ì»¬ëŸ¼ì˜ ìƒìœ„ 5ê°œë§Œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
#                 top_related[kw] = top_queries['query'].head(5).tolist()
#             else:
#                 top_related[kw] = []

#         return {
#             "analyzed_keywords": keywords_list,
#             "top_related_queries": top_related
#             # "interest_data": interest_df.to_dict() # (í•„ìš”í•˜ë‹¤ë©´ ë‚˜ì¤‘ì— ì¶”ê°€)
#         }

#     except Exception as e:
#         # (429 ì˜¤ë¥˜ ë“±ì´ ë°œìƒí•  ìˆ˜ ìˆìŒ)
#         print(f"    âŒ íŠ¸ë Œë“œ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
#         return {"error": f"íŠ¸ë Œë“œ ë¶„ì„ ì˜¤ë¥˜: {e}"}
    
# # ----------------------------------------------------
# # ê¸°ëŠ¥ 3: íŠ¸ë Œë“œ ë¶„ì„ê¸° (trend_test.pyì—ì„œ ê°€ì ¸ì˜´)
# # ----------------------------------------------------
# # analysis_tools.py íŒŒì¼ì— ì´ì–´ì„œ ì¶”ê°€í•˜ëŠ” í•¨ìˆ˜

# def get_naver_buzzwords(keyword):
#     """
#     ë„¤ì´ë²„ VIEW(ë¸”ë¡œê·¸/ì¹´í˜) íƒ­ì„ í¬ë¡¤ë§í•˜ì—¬
#     'í•¨ê»˜ ì°¾ëŠ” í‚¤ì›Œë“œ' (ì—°ê´€ íƒœê·¸) ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
#     """
#     print(f"  [analysis_tools] 3. Naver VIEW íƒ­ ì—°ê´€ í‚¤ì›Œë“œ ë¶„ì„ ì‹œì‘: {keyword}")
    
#     # 1. ë„¤ì´ë²„ VIEW íƒ­ ê²€ìƒ‰ URL
#     # (where=viewëŠ” ë¸”ë¡œê·¸/ì¹´í˜ íƒ­ì„ ì˜ë¯¸)
#     url = f"https://search.naver.com/search.naver?where=view&sm=tab_jum&query={keyword}"
    
#     # 2. (â­ï¸ì¤‘ìš”) í¬ë¡¤ë§ ì°¨ë‹¨ì„ í”¼í•˜ê¸° ìœ„í•œ 'User-Agent' í—¤ë” ì„¤ì •
#     # (ìš°ë¦¬ê°€ 'ë¸Œë¼ìš°ì €'ì¸ ì²™ ì ‘ì†í•©ë‹ˆë‹¤)
#     headers = {
#         'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
#     }

#     try:
#         # 3. 'requests'ë¡œ HTML í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
#         response = requests.get(url, headers=headers, timeout=10)
#         response.raise_for_status() # 200(ì„±ê³µ) ì½”ë“œê°€ ì•„ë‹ˆë©´ ì˜¤ë¥˜ ë°œìƒ
        
#         # 4. 'BeautifulSoup'ë¡œ HTML íŒŒì‹±(ë¶„ì„) ì¤€ë¹„
#         soup = BeautifulSoup(response.text, 'html.parser')
        
#         # 5. (â­ï¸ê°€ì¥ ì¤‘ìš”/ì·¨ì•½) "í•¨ê»˜ ì°¾ëŠ” í‚¤ì›Œë“œ"ê°€ ìˆëŠ” ì˜ì—­ ì°¾ê¸°
#         #    ë„¤ì´ë²„ëŠ” ì´ CSS ì„ íƒì(selector)ë¥¼ ìì£¼ ë°”ê¿‰ë‹ˆë‹¤.
#         #    '.keyword_box_wrap .keyword' ë˜ëŠ” '.total_tag_area .link_tag' ë“±ì„ ì‹œë„í•©ë‹ˆë‹¤.
#         related_tags_elements = soup.select('.keyword_box_wrap .keyword')
        
#         if not related_tags_elements:
#             # ë§Œì•½ ìœ„ ì„ íƒìê°€ ì‘ë™ ì•ˆ í•˜ë©´, 'ì—°ê´€ íƒœê·¸' ì˜ì—­ì„ ì‹œë„
#             related_tags_elements = soup.select('.total_tag_area .link_tag')

#         buzzwords = []
#         for tag_element in related_tags_elements:
#             # íƒœê·¸ì—ì„œ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
#             buzzword = tag_element.get_text(strip=True)
#             # '#ê´‘ì£¼ë§›ì§‘' ê°™ì€ # ê¸°í˜¸ ì œê±° (ì„ íƒ ì‚¬í•­)
#             buzzwords.append(buzzword.replace('#', ''))
            
#         if not buzzwords:
#             print("    - (ì°¸ê³ ) ì—°ê´€ í‚¤ì›Œë“œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (ë„¤ì´ë²„ êµ¬ì¡°ê°€ ë³€ê²½ë˜ì—ˆê±°ë‚˜ í‚¤ì›Œë“œ ë°ì´í„°ê°€ ì—†ìŒ)")
#             return []

#         print(f"    - Naver ì—°ê´€ í‚¤ì›Œë“œ ìˆ˜ì§‘ ì™„ë£Œ: {buzzwords[:5]}...") # (ë¡œê·¸ì—ëŠ” 5ê°œë§Œ)
        
#         # ì¤‘ë³µ ì œê±° í›„ ìƒìœ„ 10ê°œë§Œ ë°˜í™˜
#         return list(dict.fromkeys(buzzwords))[:10]

#     except Exception as e:
#         print(f"    âŒ Naver í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
#         return []
# # --- (ì´ íŒŒì¼ ìì²´ë¥¼ í…ŒìŠ¤íŠ¸í•˜ê¸° ìœ„í•œ ì½”ë“œ) ---
# if __name__ == "__main__":
#     import json
    
#     print("--- ğŸš€ 'analysis_tools.py' íŒŒì¼ ë‹¨ë… í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ---")
    
#     # 1. PDF ë¶„ì„ í…ŒìŠ¤íŠ¸
#     pdf_result = analyze_pdf("sample_plan.pdf")
#     print("\n[PDF ë¶„ì„ ê²°ê³¼ (JSON)]")
#     print(json.dumps(pdf_result, indent=2, ensure_ascii=False))
    
#     # 2. íŠ¸ë Œë“œ ë¶„ì„ í…ŒìŠ¤íŠ¸
#     trend_result = get_google_trends(["ë‹´ì–‘ ì‚°íƒ€ ì¶•ì œ", "í¬ë¦¬ìŠ¤ë§ˆìŠ¤",])
#     print("\n[íŠ¸ë Œë“œ ë¶„ì„ ê²°ê³¼ (ë”•ì…”ë„ˆë¦¬)]")
#     print(json.dumps(trend_result, indent=2, ensure_ascii=False))