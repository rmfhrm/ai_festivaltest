import fitz  # PyMuPDF
import openai
import os
from dotenv import load_dotenv
from pytrends.request import TrendReq
import pandas as pd
import json
import requests
from bs4 import BeautifulSoup
import docx  # .docx íŒŒì¼ìš©
import cloudconvert

# --- API í‚¤ ì„¤ì • (OpenAI + CloudConvert) ---
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    print("[pdf_tools] OPENAI_API_KEYë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
else:
    openai.api_key = OPENAI_API_KEY

# 2. CloudConvert API í‚¤ ë¡œë“œ
CLOUDCONVERT_API_KEY = os.getenv("CLOUDCONVERT_API_KEY")
if not CLOUDCONVERT_API_KEY:
    print("[pdf_tools] ê²½ê³ : .env íŒŒì¼ì— CLOUDCONVERT_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤.")
    print("    (HWP íŒŒì¼ ë³€í™˜ì´ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤)")
else:
    cloudconvert.configure(api_key=CLOUDCONVERT_API_KEY)
# ----------------------------------------------------

def analyze_pdf(pdf_file_path):
    """
    PDF, DOCX, HWP íŒŒì¼ ê²½ë¡œë¥¼ ë°›ì•„ì„œ, AIë¡œ ìš”ì•½í•œ JSONì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    (HWPëŠ” CloudConvert APIë¥¼ í†µí•´ PDFë¡œ ë³€í™˜í•˜ì—¬ ì²˜ë¦¬)
    """
    print(f"  [pdf_tools] 1. íŒŒì¼ ë¶„ì„ ì‹œì‘: {pdf_file_path}")
    
    full_text = ""
    if not os.path.exists(pdf_file_path):
        print(f" ì˜¤ë¥˜: '{pdf_file_path}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return {"error": "PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}

    try:
        print(f"    - íŒŒì¼ íƒ€ì… ê°ì§€ ì¤‘...")
        file_extension = os.path.splitext(pdf_file_path)[1].lower()

        # ---------------------------------
        # PDF / DOCX ì²˜ë¦¬ 
        # ---------------------------------
        if file_extension == '.pdf':
            print("    - PDF íŒŒì¼ ê°ì§€. PyMuPDFë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ...")
            doc = fitz.open(pdf_file_path)
            for page_num in range(doc.page_count):
                page = doc.load_page(page_num)
                full_text += page.get_text("text")
            doc.close()
        
        elif file_extension == '.docx':
            print("    - DOCX íŒŒì¼ ê°ì§€. python-docxë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ...")
            doc = docx.Document(pdf_file_path)
            for para in doc.paragraphs:
                full_text += para.text + "\n"
        
        # ---------------------------------
        # 3. HWP ì²˜ë¦¬ (CloudConvert APIë¡œ ì™„ì „ êµì²´)
        # ---------------------------------
        elif file_extension == '.hwp':
            print("    - HWP íŒŒì¼ ê°ì§€. CloudConvert APIë¡œ PDF ë³€í™˜ ì‹œì‘...")

            if not CLOUDCONVERT_API_KEY:
                return {"error": "HWP íŒŒì¼ì„ ì²˜ë¦¬í•˜ë ¤ë©´ .envì— CLOUDCONVERT_API_KEYê°€ í•„ìš”í•©ë‹ˆë‹¤."}

            # (1) API ì‘ì—…(Job) ìƒì„±: HWP -> PDF ë³€í™˜
            job = cloudconvert.Job.create(payload={
                "tasks": {
                    'upload-hwp': { 'operation': 'import/upload' },
                    'convert-to-pdf': { 'operation': 'convert', 'input': 'upload-hwp', 'output_format': 'pdf', 'engine': 'office' },
                    'export-pdf': { 'operation': 'export/url', 'input': 'convert-to-pdf', 'inline': True }
                }
            })

            # (2) HWP íŒŒì¼ ì—…ë¡œë“œ
            upload_task = job['tasks'][0]
            cloudconvert.Task.upload(file_name=pdf_file_path, task=upload_task)

            # (3) ì‘ì—… ì™„ë£Œ ëŒ€ê¸°
            print("    - (CloudConvert) HWP íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ. PDFë¡œ ë³€í™˜ ì¤‘...")
            job = cloudconvert.Job.wait(id=job['id'])

            # (ì˜¤ë¥˜ ìƒíƒœ í™•ì¸ ê°•í™”)
            if job.get("status") == "error":
                error_message = job.get('message', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')
                for task in job.get("tasks", []):
                    if task.get("status") == "error":
                        error_message = f"Task '{task.get('name', 'unknown')}' failed: {task.get('message', 'No details')}"
                        break
                raise Exception(f"CloudConvert API ì˜¤ë¥˜: {error_message}")

            # (4) ë³€í™˜ëœ PDFì˜ ë‹¤ìš´ë¡œë“œ URL ê°€ì ¸ì˜¤ê¸° (ë” ì•ˆì „í•œ ë°©ì‹)
            export_task = None
            for task in job.get("tasks", []):
                # ì´ë¦„ìœ¼ë¡œ 'export-pdf' ì‘ì—…ì„ ì°¾ìŒ
                if task.get("name") == "export-pdf":
                    export_task = task
                    break
            
            if not export_task:
                 raise Exception("CloudConvert job result does not contain the 'export-pdf' task.")

            if export_task.get("status") != "finished":
                 raise Exception(f"CloudConvert 'export-pdf' task did not finish successfully. Status: {export_task.get('status')}")

            result = export_task.get("result")
            if not result or not result.get("files"):
                 raise Exception("CloudConvert 'export-pdf' task result is missing or does not contain files.")

            files = result.get("files")
            if not files: # files ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸
                 raise Exception("CloudConvert 'export-pdf' task result contains an empty 'files' list.")

            # '.get()'ì„ ì‚¬ìš©í•˜ì—¬ 'url' í‚¤ì— ì•ˆì „í•˜ê²Œ ì ‘ê·¼
            pdf_url = files[0].get('url') 
            if not pdf_url:
                 # 'url' í‚¤ê°€ ì—†ì„ ê²½ìš° ëª…í™•í•œ ì˜¤ë¥˜ ë°œìƒ
                 raise KeyError("The key 'url' was not found in the first file result of the 'export-pdf' task.")
            
            # (5) ë³€í™˜ëœ PDF ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ë¡œ ë‹¤ìš´ë¡œë“œ
            print("    - (CloudConvert) PDF ë³€í™˜ ì™„ë£Œ. PDF ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì¤‘...")
            pdf_response = requests.get(pdf_url)
            pdf_response.raise_for_status()

            # (6) ë‹¤ìš´ë¡œë“œí•œ PDF ë°ì´í„°ë¥¼ 'fitz'ì—ê²Œ ì „ë‹¬
            print("    - (CloudConvert) PDF ë°ì´í„° ë¶„ì„ ì‹œì‘...")
            doc = fitz.open(stream=pdf_response.content, filetype="pdf")
            for page_num in range(doc.page_count):
                page = doc.load_page(page_num)
                full_text += page.get_text("text")
            doc.close()
        # --- â­ï¸ HWP ì²˜ë¦¬ (ë””ë²„ê¹… ë²„ì „) ë â­ï¸ ---
        # ---------------------------------
        
        else:
            print(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤: {file_extension}")
            return {"error": f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_extension}. (PDF, DOCX, HWPë§Œ ì§€ì›)"}

        print(f"    - í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ. (ì´ {len(full_text)}ì)")

        # ---------------------------------
        # AI ìš”ì•½ ìš”ì²­ (ê¸°ì¡´ê³¼ ë™ì¼)
        # ---------------------------------
        system_prompt = """
        ë‹¹ì‹ ì€ ì¶•ì œ ê¸°íšì„œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
        ì‚¬ìš©ìê°€ ì œê³µí•˜ëŠ” ê¸°íšì„œ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬,
        ì•„ë˜ í•­ëª©ì— í•´ë‹¹í•˜ëŠ” 'êµ¬ì²´ì ì¸ ìƒì„¸ ë‚´ìš©'ì„ ì¶”ì¶œí•˜ê³ 
        ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”.
        
        [ì¤‘ìš” ê·œì¹™]
        1. ì˜¤ì§ ì•„ë˜ ëª©ë¡ì—ì„œ ìš”ì²­ëœ í•­ëª©('title', 'date', 'location' ë“±)ë§Œ ì¶”ì¶œí•˜ì„¸ìš”.
        2. 'ì˜ˆì‚°', 'ì‚¬ì—…ë¹„', 'ì´ê¸ˆì•¡' ë“± **ê¸ˆì•¡(ëˆ)ê³¼ ê´€ë ¨ëœ ëª¨ë“  ì •ë³´**ëŠ” 
           ê·¸ê²ƒì´ ì–´ë–¤ í•­ëª©ì´ë“  **ì ˆëŒ€ë¡œ** ìš”ì•½ì— í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
        3. 'ì•ˆì „ ëŒ€ì±…(Safety Measures)', 'í–‰ì • ì‚¬í•­', 'ì…ì°°' ë“± 
           ëª©ë¡ì— ì—†ëŠ” ë‹¤ë¥¸ ì •ë³´ë„ **ì ˆëŒ€ë¡œ** ìš”ì•½ì— í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
        
        --- (ì¶”ì¶œí•  í•­ëª© ëª©ë¡) ---
        - "title": ì¶•ì œ ê³µì‹ ì œëª©
        - "date": ì¶•ì œê°€ ì—´ë¦¬ëŠ” ì •í™•í•œ ë‚ ì§œì™€ ê¸°ê°„
        - "location": ì¶•ì œê°€ ì—´ë¦¬ëŠ” êµ¬ì²´ì ì¸ ì¥ì†Œ
        - "host": ì£¼ìµœ ê¸°ê´€
        - "organizer": ì£¼ê´€ ê¸°ê´€
        - "targetAudience": ì¶•ì œì˜ ì£¼ìš” ëŒ€ìƒ ê³ ê° (ì˜ˆ: 'ê°€ì¡± ë‹¨ìœ„ ë°©ë¬¸ê°', '2030 ì—°ì¸', 'ì–´ë¦°ì´'). 'ì£¼ìš” íƒ€ê¹ƒ' ë˜ëŠ” 'ê³ ê°ì¸µ' ê°™ì€ ë‹¨ì–´ ê·¼ì²˜ë¥¼ ì°¾ì•„ë³´ì„¸ìš”.
        - "summary": ì¶•ì œì˜ ëª©ì ê³¼ í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½
        - "programs": ë°©ë¬¸ê°ì´ 'ì²´í—˜'í•  ìˆ˜ ìˆëŠ” ì£¼ìš” í”„ë¡œê·¸ë¨ì˜ 'êµ¬ì²´ì ì¸ ë‚´ìš©' (ë¦¬ìŠ¤íŠ¸). (ì£¼ì˜: 'í”„ë¡œê·¸ë¨'ì´ë¼ëŠ” ì œëª©ì˜ ëª©ì°¨ë¿ë§Œ ì•„ë‹ˆë¼, ê·¸ 'ìƒì„¸ ë‚´ìš©'ì„ ì°¾ì•„ì£¼ì„¸ìš”.)
        - "events": ì¶•ì œ ê¸°ê°„ ì¤‘ ì—´ë¦¬ëŠ” 'íŠ¹ë³„ ì´ë²¤íŠ¸'ì˜ 'êµ¬ì²´ì ì¸ ë‚´ìš©' (ë¦¬ìŠ¤íŠ¸). (ì˜ˆ: 'ê°œë§‰ í¼í¬ë¨¼ìŠ¤', 'ì‚°íƒ€ ì´ë²¤íŠ¸ ìš´ì˜'). (ì£¼ì˜: 'ì´ë²¤íŠ¸'ë¼ëŠ” ì œëª©ì˜ ëª©ì°¨ë¿ë§Œ ì•„ë‹ˆë¼, ê·¸ 'ìƒì„¸ ë‚´ìš©'ì„ ì°¾ì•„ì£¼ì„¸ìš”.)
        - "visualKeywords": ì¹´ë“œë‰´ìŠ¤ ë””ìì¸ì— ì°¸ê³ í•  ë§Œí•œ ì‹œê°ì  í‚¤ì›Œë“œ (ì˜ˆ: "ì•¼ê°„ ì¡°ëª…", "í¬ë¦¬ìŠ¤ë§ˆìŠ¤ íŠ¸ë¦¬", "ì‚°íƒ€") (ë¦¬ìŠ¤íŠ¸)
        - "contactInfo": ë°©ë¬¸ê°ì´ ë¬¸ì˜í•  ìˆ˜ ìˆëŠ” ì „í™”ë²ˆí˜¸ ë˜ëŠ” ê³µì‹ ì›¹ì‚¬ì´íŠ¸ ì£¼ì†Œ
        - "directions": ë°©ë¬¸ê°ì´ ì¶•ì œ ì¥ì†Œì— 'ì˜¤ì‹œëŠ” ê¸¸' (ì˜ˆ: 'xx ICì—ì„œ 10ë¶„', 'ë‹´ì–‘ ë²„ìŠ¤í„°ë¯¸ë„ì—ì„œ 5ë²ˆ ë²„ìŠ¤', 'ì£¼ì°¨: ë©”íƒ€ëœë“œ ì£¼ì°¨ì¥ ì´ìš©').
                       (ì£¼ì˜: 'ì‚¬ì—… ì§€ì‹œ'ë‚˜ 'ì œì•ˆì„œ ì ‘ìˆ˜' ë‚´ìš©ì´ ì•„ë‹˜. ë°©ë¬¸ê°ìš© êµí†µ/ì£¼ì°¨ ì •ë³´ê°€ ëª…í™•íˆ ì—†ìœ¼ë©´ "ì •ë³´ ì—†ìŒ"ìœ¼ë¡œ í‘œê¸°)
        
        ë§Œì•½ í…ìŠ¤íŠ¸ì—ì„œ íŠ¹ì • ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´, í•´ë‹¹ ê°’ì€ "ì •ë³´ ì—†ìŒ"ìœ¼ë¡œ í‘œê¸°í•˜ì„¸ìš”.

        [ìµœì¢… í™•ì¸ ê·œì¹™]
        ì‘ë‹µí•˜ê¸° ì „, ë‹¹ì‹ ì´ ìƒì„±í•œ JSONì„ ë‹¤ì‹œ í•œë²ˆ í™•ì¸í•˜ì„¸ìš”.
        JSON ë‚´ë¶€ì— 'ì˜ˆì‚°', 'ì‚¬ì—…ë¹„' ë“± **ê¸ˆì•¡(ëˆ)ê³¼ ê´€ë ¨ëœ ë‚´ìš©**ì´ë‚˜, 
        'ì•ˆì „ ëŒ€ì±…' ë“± --- (ì¶”ì¶œí•  í•­ëª© ëª©ë¡) ---ì— ì—†ì—ˆë˜ í•­ëª©ì´ í¬í•¨ë˜ì–´ ìˆë‚˜ìš”?
        ë§Œì•½ ê·¸ë ‡ë‹¤ë©´, ê·¸ í•­ëª©ë“¤ì„ **ë°˜ë“œì‹œ ì‚­ì œ**í•˜ê³ 
        ì˜¤ì§ 'title'ë¶€í„° 'directions'ê¹Œì§€ì˜ í•­ëª©ë§Œ í¬í•¨í•´ì„œ ì‘ë‹µí•˜ì„¸ìš”.
        """
        
        user_prompt = f"ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ JSONìœ¼ë¡œ ìš”ì•½í•´ì¤˜:\n\n{full_text[:10000]}"

        client = openai.OpenAI()
        response = client.chat.completions.create(
            model="gpt-4-turbo",  # (í…ŒìŠ¤íŠ¸ ê²°ê³¼ gpt-3.5-turboë³´ë‹¤ gpt-4-turboê°€ í›¨ì”¬ ì•ˆì •ì ì…ë‹ˆë‹¤)
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            response_format={"type": "json_object"},
            max_tokens= 4000
        )
        
        ai_response_json_string = response.choices[0].message.content
        print("    - AI ìš”ì•½ ì™„ë£Œ.")
        
        # JSON ë¬¸ìì—´ì„ Python ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜í•´ì„œ ë°˜í™˜
        return json.loads(ai_response_json_string) 

    except Exception as e:
        print(f" PDF ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return {"error": f"PDF ë¶„ì„ ì˜¤ë¥˜: {e}"}

# # ----------------------------------------------------
# # ê¸°ëŠ¥ 2: íŠ¸ë Œë“œ ë¶„ì„ê¸° (trend_test.pyì—ì„œ ê°€ì ¸ì˜´)
# # ----------------------------------------------------
# def get_google_trends(keywords_list):
#     """
#     í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„ì„œ, Google íŠ¸ë Œë“œ ë°ì´í„°ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
#     """
#     print(f"  [analysis_tools] 2. Google íŠ¸ë Œë“œ ë¶„ì„ ì‹œì‘: {keywords_list}")
    
#     try:
#         pytrends = TrendReq(hl='ko-KR', tz=540)
#         pytrends.build_payload(keywords_list, cat=0, timeframe='today 12-m', geo='KR')
        
#         # (1) ì‹œê°„ë³„ ê´€ì‹¬ë„
#         interest_df = pytrends.interest_over_time()
        
#         # (2) ì—°ê´€ ê²€ìƒ‰ì–´
#         related_queries_dict = pytrends.related_queries()
        
#         print("    - íŠ¸ë Œë“œ ë¶„ì„ ì™„ë£Œ.")
        
#         # (â€» DataFrameì€ JSONìœ¼ë¡œ ë°”ë¡œ ë³´ë‚´ê¸° ê¹Œë‹¤ë¡œìš°ë¯€ë¡œ,
#         #    ë‚˜ì¤‘ì— í•„ìš”í•œ 'ì—°ê´€ ê²€ìƒ‰ì–´'ë§Œ ë¨¼ì € ê°€ê³µí•´ì„œ ë°˜í™˜í•©ë‹ˆë‹¤.)
        
#         top_related = {}
#         for kw in keywords_list:
#             top_queries = related_queries_dict.get(kw, {}).get('top')
#             if top_queries is not None and not top_queries.empty:
#                 # 'query' ì»¬ëŸ¼ì˜ ìƒìœ„ 5ê°œë§Œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
#                 top_related[kw] = top_queries['query'].head(5).tolist()
#             else:
#                 top_related[kw] = []

#         return {
#             "analyzed_keywords": keywords_list,
#             "top_related_queries": top_related
#             # "interest_data": interest_df.to_dict() # (í•„ìš”í•˜ë‹¤ë©´ ë‚˜ì¤‘ì— ì¶”ê°€)
#         }

#     except Exception as e:
#         # (429 ì˜¤ë¥˜ ë“±ì´ ë°œìƒí•  ìˆ˜ ìˆìŒ)
#         print(f"    âŒ íŠ¸ë Œë“œ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
#         return {"error": f"íŠ¸ë Œë“œ ë¶„ì„ ì˜¤ë¥˜: {e}"}
    
# # ----------------------------------------------------
# # ê¸°ëŠ¥ 3: íŠ¸ë Œë“œ ë¶„ì„ê¸° (trend_test.pyì—ì„œ ê°€ì ¸ì˜´)
# # ----------------------------------------------------
# # analysis_tools.py íŒŒì¼ì— ì´ì–´ì„œ ì¶”ê°€í•˜ëŠ” í•¨ìˆ˜

# def get_naver_buzzwords(keyword):
#     """
#     ë„¤ì´ë²„ VIEW(ë¸”ë¡œê·¸/ì¹´í˜) íƒ­ì„ í¬ë¡¤ë§í•˜ì—¬
#     'í•¨ê»˜ ì°¾ëŠ” í‚¤ì›Œë“œ' (ì—°ê´€ íƒœê·¸) ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
#     """
#     print(f"  [analysis_tools] 3. Naver VIEW íƒ­ ì—°ê´€ í‚¤ì›Œë“œ ë¶„ì„ ì‹œì‘: {keyword}")
    
#     # 1. ë„¤ì´ë²„ VIEW íƒ­ ê²€ìƒ‰ URL
#     # (where=viewëŠ” ë¸”ë¡œê·¸/ì¹´í˜ íƒ­ì„ ì˜ë¯¸)
#     url = f"https://search.naver.com/search.naver?where=view&sm=tab_jum&query={keyword}"
    
#     # 2. (â­ï¸ì¤‘ìš”) í¬ë¡¤ë§ ì°¨ë‹¨ì„ í”¼í•˜ê¸° ìœ„í•œ 'User-Agent' í—¤ë” ì„¤ì •
#     # (ìš°ë¦¬ê°€ 'ë¸Œë¼ìš°ì €'ì¸ ì²™ ì ‘ì†í•©ë‹ˆë‹¤)
#     headers = {
#         'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
#     }

#     try:
#         # 3. 'requests'ë¡œ HTML í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
#         response = requests.get(url, headers=headers, timeout=10)
#         response.raise_for_status() # 200(ì„±ê³µ) ì½”ë“œê°€ ì•„ë‹ˆë©´ ì˜¤ë¥˜ ë°œìƒ
        
#         # 4. 'BeautifulSoup'ë¡œ HTML íŒŒì‹±(ë¶„ì„) ì¤€ë¹„
#         soup = BeautifulSoup(response.text, 'html.parser')
        
#         # 5. (â­ï¸ê°€ì¥ ì¤‘ìš”/ì·¨ì•½) "í•¨ê»˜ ì°¾ëŠ” í‚¤ì›Œë“œ"ê°€ ìˆëŠ” ì˜ì—­ ì°¾ê¸°
#         #    ë„¤ì´ë²„ëŠ” ì´ CSS ì„ íƒì(selector)ë¥¼ ìì£¼ ë°”ê¿‰ë‹ˆë‹¤.
#         #    '.keyword_box_wrap .keyword' ë˜ëŠ” '.total_tag_area .link_tag' ë“±ì„ ì‹œë„í•©ë‹ˆë‹¤.
#         related_tags_elements = soup.select('.keyword_box_wrap .keyword')
        
#         if not related_tags_elements:
#             # ë§Œì•½ ìœ„ ì„ íƒìê°€ ì‘ë™ ì•ˆ í•˜ë©´, 'ì—°ê´€ íƒœê·¸' ì˜ì—­ì„ ì‹œë„
#             related_tags_elements = soup.select('.total_tag_area .link_tag')

#         buzzwords = []
#         for tag_element in related_tags_elements:
#             # íƒœê·¸ì—ì„œ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
#             buzzword = tag_element.get_text(strip=True)
#             # '#ê´‘ì£¼ë§›ì§‘' ê°™ì€ # ê¸°í˜¸ ì œê±° (ì„ íƒ ì‚¬í•­)
#             buzzwords.append(buzzword.replace('#', ''))
            
#         if not buzzwords:
#             print("    - (ì°¸ê³ ) ì—°ê´€ í‚¤ì›Œë“œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (ë„¤ì´ë²„ êµ¬ì¡°ê°€ ë³€ê²½ë˜ì—ˆê±°ë‚˜ í‚¤ì›Œë“œ ë°ì´í„°ê°€ ì—†ìŒ)")
#             return []

#         print(f"    - Naver ì—°ê´€ í‚¤ì›Œë“œ ìˆ˜ì§‘ ì™„ë£Œ: {buzzwords[:5]}...") # (ë¡œê·¸ì—ëŠ” 5ê°œë§Œ)
        
#         # ì¤‘ë³µ ì œê±° í›„ ìƒìœ„ 10ê°œë§Œ ë°˜í™˜
#         return list(dict.fromkeys(buzzwords))[:10]

#     except Exception as e:
#         print(f"    âŒ Naver í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
#         return []
# # --- (ì´ íŒŒì¼ ìì²´ë¥¼ í…ŒìŠ¤íŠ¸í•˜ê¸° ìœ„í•œ ì½”ë“œ) ---
# if __name__ == "__main__":
#     import json
    
#     print("--- ğŸš€ 'analysis_tools.py' íŒŒì¼ ë‹¨ë… í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ---")
    
#     # 1. PDF ë¶„ì„ í…ŒìŠ¤íŠ¸
#     pdf_result = analyze_pdf("sample_plan.pdf")
#     print("\n[PDF ë¶„ì„ ê²°ê³¼ (JSON)]")
#     print(json.dumps(pdf_result, indent=2, ensure_ascii=False))
    
#     # 2. íŠ¸ë Œë“œ ë¶„ì„ í…ŒìŠ¤íŠ¸
#     trend_result = get_google_trends(["ë‹´ì–‘ ì‚°íƒ€ ì¶•ì œ", "í¬ë¦¬ìŠ¤ë§ˆìŠ¤",])
#     print("\n[íŠ¸ë Œë“œ ë¶„ì„ ê²°ê³¼ (ë”•ì…”ë„ˆë¦¬)]")
#     print(json.dumps(trend_result, indent=2, ensure_ascii=False))